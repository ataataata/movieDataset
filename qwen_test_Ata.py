#!/usr/bin/env python3
"""
identify_speakers_qwen.py
-------------------------
Scan every .wav under a directory, ask Qwen-2-Audio-7B-Instruct
â€˜Can you figure out who this speaker is?â€™ and write incremental
results to JSON so progress survives crashes or OOMs.

Tested on macOS 14 / Apple-silicon (M3) with:
  â€¢ Python 3.12
  â€¢ PyTorch 2.3 + mps backend
  â€¢ transformers >= 4.54
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict

import librosa
import torch
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
DEFAULT_MODEL = "Qwen/Qwen2-Audio-7B-Instruct"
DEFAULT_INPUT_DIR = Path("/Users/atacinargenc/Desktop/github/movieDataset/Lines")
DEFAULT_OUTPUT_FILE = Path("qwen_test.json")

# Work out the best device: Apple-silicon GPU if available, else CPU
DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def load_model(model_name: str = DEFAULT_MODEL):
    print(f"ğŸ”„ Loading {model_name} on {DEVICE.upper()} â€¦")

    # Processor (tokeniser + feature extractor)
    processor = AutoProcessor.from_pretrained(model_name)

    # Model â€“ float16 on M-series, full-precision elsewhere
    dtype = torch.float16 if DEVICE == "mps" else None
    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=dtype,
    ).to(DEVICE)
    model.eval()

    return processor, model


def analyse_clip(wav_path: Path, processor, model) -> str:
    """Run a single WAV through the model and return its answer."""
    # Load audio at the modelâ€™s expected sample rate
    target_sr = processor.feature_extractor.sampling_rate
    audio, _ = librosa.load(wav_path, sr=target_sr, mono=True)

    # Limit each clip to 30 s to keep memory in check
    max_len = 30 * target_sr
    audio = audio[:max_len]

    print(f"   ğŸ” Audio shape: {audio.shape}, SR: {target_sr}")

    # Build multi-modal chat template
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": audio},
                {"type": "text", "text": "Can you figure out who this speaker is?"},
            ],
        }
    ]
    text_prompt = processor.apply_chat_template(
        conversation, add_generation_prompt=True, tokenize=False
    )
    print(f"   ğŸ” Generated prompt length: {len(text_prompt)} characters")

    # NEW: use the *singular* kwarg `audio=` (tuple of (array, sample_rate))
    inputs = processor(
        text=text_prompt,
        audio=[audio],
        sampling_rate=target_sr, 
        return_tensors="pt",
        padding=True,
    ).to(DEVICE)

    input_len = inputs.input_ids.size(1)
    print(f"   ğŸ” Input tokens: {input_len}")

    # Generate
    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_new_tokens=8,
            do_sample=True,
            temperature=0.7,
        )

    # Strip the prompt so we keep only the answer
    generated = generated[:, input_len:]
    response = processor.batch_decode(
        generated, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0].strip()

    print(f"   âœ… Generated: '{response[:100]}{'â€¦' if len(response) > 100 else ''}'")
    return response or "No response generated"


def traverse_and_analyse(
    root_dir: Path,
    out_path: Path,
    processor,
    model,
) -> Dict[str, str]:
    """Walk the directory tree, run every WAV, and dump incremental JSON."""
    results: Dict[str, str] = {}

    # Resume from previous run if possible
    if out_path.exists():
        try:
            with out_path.open() as f:
                results = json.load(f)
            print(f"ğŸ“‚ Loaded {len(results)} existing results from {out_path}")
        except Exception as e:
            print(f"âš ï¸  Could not read existing results: {e}")

    wav_paths = sorted(root_dir.rglob("*.wav"))
    print(f"ğŸ” Found {len(wav_paths)} .wav files under {root_dir}")

    for idx, wav_path in enumerate(wav_paths, 1):
        rel_path = wav_path.relative_to(root_dir).as_posix()

        if rel_path in results:
            print(f"[{idx}/{len(wav_paths)}] â­ï¸  {rel_path} (already done)")
            continue

        print(f"[{idx}/{len(wav_paths)}] ğŸ§  {rel_path}")
        try:
            guess = analyse_clip(wav_path, processor, model)
        except Exception as exc:
            print(f"âš ï¸  Error on '{rel_path}': {exc}")
            guess = f"ERROR: {exc}"

        results[rel_path] = guess
        out_path.write_text(json.dumps(results, indent=2, ensure_ascii=False))

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def parse_args():
    p = argparse.ArgumentParser(
        description="Identify speakers with Qwen-2-Audio-7B-Instruct."
    )
    p.add_argument("--input_dir", type=Path, default=DEFAULT_INPUT_DIR,
                   help=f".wav root (default: {DEFAULT_INPUT_DIR})")
    p.add_argument("--output_file", type=Path, default=DEFAULT_OUTPUT_FILE,
                   help=f"JSON results file (default: {DEFAULT_OUTPUT_FILE})")
    p.add_argument("--model_name", default=DEFAULT_MODEL,
                   help="HF model name or local checkpoint")
    return p.parse_args()


def main():
    args = parse_args()
    args.output_file.parent.mkdir(parents=True, exist_ok=True)

    processor, model = load_model(args.model_name)
    traverse_and_analyse(
        args.input_dir.expanduser(),
        args.output_file,
        processor,
        model,
    )
    print(f"\nâœ…  All done. Results saved to {args.output_file}")


if __name__ == "__main__":
    main()
